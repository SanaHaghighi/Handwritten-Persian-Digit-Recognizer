{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 350 files belonging to 10 classes.\n",
      "Using 315 files for training.\n",
      "Found 350 files belonging to 10 classes.\n",
      "Using 35 files for validation.\n",
      "\n",
      "Start of Training Epoch 0\n",
      "Accuracy over epoch 0.09841269999742508\n",
      "\n",
      "Start of Training Epoch 1\n",
      "Accuracy over epoch 0.2380952388048172\n",
      "\n",
      "Start of Training Epoch 2\n",
      "Accuracy over epoch 0.42222222685813904\n",
      "\n",
      "Start of Training Epoch 3\n",
      "Accuracy over epoch 0.5746031999588013\n",
      "\n",
      "Start of Training Epoch 4\n",
      "Accuracy over epoch 0.6380952596664429\n",
      "\n",
      "Start of Training Epoch 5\n",
      "Accuracy over epoch 0.7650793790817261\n",
      "\n",
      "Start of Training Epoch 6\n",
      "Accuracy over epoch 0.8253968358039856\n",
      "\n",
      "Start of Training Epoch 7\n",
      "Accuracy over epoch 0.8952381014823914\n",
      "\n",
      "Start of Training Epoch 8\n",
      "Accuracy over epoch 0.9269841313362122\n",
      "\n",
      "Start of Training Epoch 9\n",
      "Accuracy over epoch 0.9396825432777405\n",
      "Epoch 1/10\n",
      "158/158 - 2s - loss: 0.1515 - accuracy: 0.9683 - 2s/epoch - 14ms/step\n",
      "Epoch 2/10\n",
      "158/158 - 2s - loss: 0.1334 - accuracy: 0.9683 - 2s/epoch - 14ms/step\n",
      "Epoch 3/10\n",
      "158/158 - 2s - loss: 0.0857 - accuracy: 0.9905 - 2s/epoch - 14ms/step\n",
      "Epoch 4/10\n",
      "158/158 - 1s - loss: 0.0950 - accuracy: 0.9714 - 1s/epoch - 7ms/step\n",
      "Epoch 5/10\n",
      "158/158 - 1s - loss: 0.0767 - accuracy: 0.9841 - 1s/epoch - 9ms/step\n",
      "Epoch 6/10\n",
      "158/158 - 1s - loss: 0.0656 - accuracy: 0.9841 - 1s/epoch - 7ms/step\n",
      "Epoch 7/10\n",
      "158/158 - 1s - loss: 0.2031 - accuracy: 0.9333 - 1s/epoch - 7ms/step\n",
      "Epoch 8/10\n",
      "158/158 - 2s - loss: 0.0569 - accuracy: 0.9905 - 2s/epoch - 13ms/step\n",
      "Epoch 9/10\n",
      "158/158 - 1s - loss: 0.0237 - accuracy: 0.9968 - 1s/epoch - 8ms/step\n",
      "Epoch 10/10\n",
      "158/158 - 2s - loss: 0.0437 - accuracy: 0.9841 - 2s/epoch - 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x149000ad2b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports needed\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "img_height = 28\n",
    "img_width = 28\n",
    "batch_size = 2\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input((28, 28, 1)),\n",
    "        layers.Conv2D(16, 3, padding=\"same\"),\n",
    "        layers.Conv2D(32, 3, padding=\"same\"),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#                      METHOD 1\n",
    "# ==================================================== #\n",
    "#             Using dataset_from_directory             #\n",
    "# ==================================================== #\n",
    "ds_train = tf.keras.preprocessing.image_dataset_from_directory(\"./mnist_dataset/\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",  # categorical, binary\n",
    "    # class_names=['0', '1', '2', '3', ...]\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),  # reshape if not in this size\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.1,\n",
    "    subset=\"training\",\n",
    ")\n",
    "\n",
    "ds_validation = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"./mnist_dataset/\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",  # categorical, binary\n",
    "    # class_names=['0', '1', '2', '3', ...]\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),  # reshape if not in this size\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.1,\n",
    "    subset=\"validation\",\n",
    ")\n",
    "\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomContrast(0.1),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "ds_train = ds_train.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "\n",
    "# Custom Loop\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define your metric\n",
    "acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Custom Loop\n",
    "for epoch in range(10): \n",
    "    print(f\"\\nStart of Training Epoch {epoch}\")\n",
    "    for x, y in ds_train:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x, training=True)\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=y, logits=y_pred\n",
    "                )\n",
    "            )\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "        acc_metric.update_state(y, y_pred)\n",
    "\n",
    "    train_acc = acc_metric.result()\n",
    "    print(f\"Accuracy over epoch {train_acc}\")\n",
    "    acc_metric.reset_states()\n",
    "    \n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=[keras.losses.SparseCategoricalCrossentropy(from_logits=True),],\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(ds_train, epochs=10, verbose=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of Training Epoch 0\n",
      "Accuracy over epoch 0.9920634627342224\n",
      "\n",
      "Start of Training Epoch 1\n",
      "Accuracy over epoch 0.9936507940292358\n",
      "\n",
      "Start of Training Epoch 2\n",
      "Accuracy over epoch 0.9682539701461792\n",
      "\n",
      "Start of Training Epoch 3\n",
      "Accuracy over epoch 0.961904764175415\n",
      "\n",
      "Start of Training Epoch 4\n",
      "Accuracy over epoch 0.9873015880584717\n",
      "\n",
      "Start of Training Epoch 5\n",
      "Accuracy over epoch 0.9746031761169434\n",
      "\n",
      "Start of Training Epoch 6\n",
      "Accuracy over epoch 0.9968253970146179\n",
      "\n",
      "Start of Training Epoch 7\n",
      "Accuracy over epoch 0.9968253970146179\n",
      "\n",
      "Start of Training Epoch 8\n",
      "Accuracy over epoch 1.0\n",
      "\n",
      "Start of Training Epoch 9\n",
      "Accuracy over epoch 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Custom Loop\n",
    "for epoch in range(10): \n",
    "    print(f\"\\nStart of Training Epoch {epoch}\")\n",
    "    for x, y in ds_train:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x, training=True)\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=y, logits=y_pred\n",
    "                )\n",
    "            )\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "        acc_metric.update_state(y, y_pred)\n",
    "        \n",
    "        train_losses.append(loss.numpy())\n",
    "\n",
    "    train_acc = acc_metric.result()\n",
    "    print(f\"Accuracy over epoch {train_acc}\")\n",
    "    acc_metric.reset_states()\n",
    "    \n",
    "    for x_val, y_val in ds_validation:\n",
    "        y_val_pred = model(x_val, training=False)\n",
    "        val_loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=y_val, logits=y_val_pred\n",
    "            )\n",
    "        )\n",
    "        val_losses.append(val_loss.numpy())\n",
    "    \n",
    "# Plot the training and validation losses\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 350 images belonging to 10 classes.\n",
      "Found 350 images belonging to 10 classes.\n",
      "Epoch 1/10\n",
      "175/175 [==============================] - 3s 11ms/step - loss: 2.4396 - accuracy: 0.0914 - val_loss: 2.1866 - val_accuracy: 0.2714\n",
      "Epoch 2/10\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 2.2682 - accuracy: 0.1800 - val_loss: 1.9558 - val_accuracy: 0.3400\n",
      "Epoch 3/10\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 2.0652 - accuracy: 0.2486 - val_loss: 1.6702 - val_accuracy: 0.4486\n",
      "Epoch 4/10\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 1.9149 - accuracy: 0.3571 - val_loss: 1.7208 - val_accuracy: 0.3600\n",
      "Epoch 5/10\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 1.7814 - accuracy: 0.3943 - val_loss: 1.4206 - val_accuracy: 0.4714\n",
      "Epoch 6/10\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 1.5948 - accuracy: 0.4371 - val_loss: 1.1078 - val_accuracy: 0.6343\n",
      "Epoch 7/10\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 1.4416 - accuracy: 0.5371 - val_loss: 0.9899 - val_accuracy: 0.6743\n",
      "Epoch 8/10\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 1.4100 - accuracy: 0.5429 - val_loss: 0.9139 - val_accuracy: 0.7229\n",
      "Epoch 9/10\n",
      "175/175 [==============================] - 2s 13ms/step - loss: 1.2796 - accuracy: 0.5543 - val_loss: 0.8437 - val_accuracy: 0.7171\n",
      "Epoch 10/10\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 1.2203 - accuracy: 0.5743 - val_loss: 1.2135 - val_accuracy: 0.6257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1490af02640>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "img_height = 28\n",
    "img_width = 28\n",
    "batch_size = 2\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input((28, 28, 1)),\n",
    "        layers.Conv2D(16, 3, padding=\"same\"),\n",
    "        layers.Conv2D(32, 3, padding=\"same\"),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "train_data_dir = \"./mnist_dataset/\"\n",
    "validation_data_dir = \"./mnist_dataset/\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=[keras.losses.CategoricalCrossentropy(from_logits=True)],\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the pretrained model on MNIST\n",
    "pretrained_model = keras.models.load_model(\"./mnist_dataset/\")\n",
    "\n",
    "# Freeze all layers in the pretrained model\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a new output layer for the new dataset\n",
    "num_classes = 10 # Number of classes in the new dataset\n",
    "output = layers.Dense(num_classes)(pretrained_model.layers[-2].output)\n",
    "\n",
    "# Create the new model\n",
    "model = keras.Model(pretrained_model.input, output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Load and preprocess your own dataset (Persian dataset)\n",
    "data_dir = \"./your_persian_dataset_directory\"\n",
    "train_data_dir = os.path.join(data_dir, \"train\")\n",
    "validation_data_dir = os.path.join(data_dir, \"validation\")\n",
    "\n",
    "# Use ImageDataGenerator for data preprocessing\n",
    "data_augmentation = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "train_generator = data_augmentation.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(28, 28),\n",
    "    batch_size=32,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"sparse\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = data_augmentation.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(28, 28),\n",
    "    batch_size=32,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"sparse\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Train the model on the Persian dataset\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
